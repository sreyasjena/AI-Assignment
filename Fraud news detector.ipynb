{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91b7bed7-a253-417b-8ecc-cd7cc0d30aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4c16908-27aa-4a5d-b8a4-42998208203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = pd.read_csv(\"C:/Users/sreya/Downloads/archive (1)/Fake.csv\")\n",
    "true = pd.read_csv(\"C:/Users/sreya/Downloads/archive (1)/True.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17b741be-6070-4286-acd8-984467696619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Yearâ€™...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obamaâ€™s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Yearâ€™...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obamaâ€™s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  \n",
       "0  December 31, 2017  \n",
       "1  December 31, 2017  \n",
       "2  December 30, 2017  \n",
       "3  December 29, 2017  \n",
       "4  December 25, 2017  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7f82603-b1b8-4561-b7e7-fdde6e10acce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  \n",
       "0  December 31, 2017   \n",
       "1  December 29, 2017   \n",
       "2  December 31, 2017   \n",
       "3  December 30, 2017   \n",
       "4  December 29, 2017   "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cd5bce2-5bc1-4094-89bc-f2150116f857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23481 21417\n"
     ]
    }
   ],
   "source": [
    "print(len(fake),len(true)) # to check the length of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b9bb717-9c00-40ea-8f4a-765859ad2430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sreya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sreya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "try:\n",
    "    import nltk\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except Exception:\n",
    "    print(\"Downloading minimal NLTK data (punkt, stopwords). This may require internet access...\")\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66b7cd2c-424f-4a5f-bab4-9a794bba9c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return\n",
    "# return new lines      \n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "# remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "# keep basic punctuation, remove strange characters\n",
    "    text = re.sub(r\"[^A-Za-z0-9\\.,!?:;\\'\\\"\\s-]\", ' ', text)\n",
    "# collapse whitespace\n",
    "    text = re.sub(r\"\\s+\", ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f10a01fa-bb67-4a8a-b4af-831448116a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine title and text when available\n",
    "fake['content'] = (fake.get('title', '').fillna('') + '. ' + fake.get('text', '').fillna('')).apply(clean_text)\n",
    "true['content'] = (true.get('title', '').fillna('') + '. ' + true.get('text', '').fillna('')).apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9536763a-c85b-4e62-8630-b693914b70da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with labels: fake=1, real=0\n",
    "fake = fake.assign(label=1)\n",
    "true = true.assign(label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e80c70b9-99b7-4cfb-91d2-275375a5f2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two dataframes of fake and true news and shuffle the rows\n",
    "df = pd.concat([fake[['content','label']], true[['content','label']]], ignore_index=True)\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abb64412-ed03-4780-8a7a-62dc37329b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ben Stein Calls Out 9th Circuit Court: Committ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump drops Steve Bannon from National Securit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Puerto Rico expects U.S. to lift Jones Act shi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OOPS: Trump Just Accidentally Confirmed He Lea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donald Trump heads for Scotland to reopen a go...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  label\n",
       "0  Ben Stein Calls Out 9th Circuit Court: Committ...      1\n",
       "1  Trump drops Steve Bannon from National Securit...      0\n",
       "2  Puerto Rico expects U.S. to lift Jones Act shi...      0\n",
       "3  OOPS: Trump Just Accidentally Confirmed He Lea...      1\n",
       "4  Donald Trump heads for Scotland to reopen a go...      0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36f24388-9216-40df-aa5d-d7a2616598d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop empty columns\n",
    "df = df[df['content'].str.strip() != ''].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "312eb49a-e635-4156-96bd-2b448c58ef52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44889 entries, 0 to 44888\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   content  44889 non-null  object\n",
      " 1   label    44889 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 701.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cff55bd1-ae31-472a-a443-d1d8bf303a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (44889, 2)\n",
      "label\n",
      "1    23472\n",
      "0    21417\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Combined dataset shape: {df.shape}\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc909baf-b1e6-4070-b4d2-e564571caeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ben Stein Calls Out 9th Circuit Court: Committed a Coup d tat Against the Constitution. 21st Century Wire says Ben Stein, reputable professor from, Pepperdine University also of some Hollywood fame appearing in TV shows and films such as Ferris Bueller s Day Off made some provocative statements on Judge Jeanine Pirro s show recently. While discussing the halt that was imposed on President Trump s Executive Order on travel. Stein referred to the judgement by the 9th Circuit Court in Washington state as a Coup d tat against the executive branch and against the constitution. Stein went on to call the Judges in Seattle political puppets and the judiciary political pawns. Watch the interview below for the complete statements and note the stark contrast to the rhetoric of the leftist media and pundits who neglect to note that no court has ever blocked any Presidential orders in immigration in the past or discuss the legal efficacy of the halt or the actual text of the Executive Order.READ MORE TRUMP NEWS AT: 21st Century Wire Trump FilesSUPPORT OUR WORK BY SUBSCRIBING BECOMING A MEMBER 21WIRE.TV\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0,'content'][0:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da1c36-8bc1-46a0-814e-2301a708e99b",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "933df9a5-f815-41f7-9874-e11a0c0a98c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train (35911) and test (8978) to artifacts\n"
     ]
    }
   ],
   "source": [
    "# Purpose: Create a reproducible split and save to artifacts/ for later steps\n",
    "from sklearn.model_selection import train_test_split\n",
    "OUT = Path(\"artifacts\")\n",
    "OUT.mkdir(exist_ok=True)\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.20, stratify=df['label'], random_state=42)\n",
    "train_df.to_csv(OUT / 'train.csv', index=False)\n",
    "test_df.to_csv(OUT / 'test.csv', index=False)\n",
    "print(f\"Saved train ({len(train_df)}) and test ({len(test_df)}) to {OUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "583c0b89-dcfd-42bf-935c-98143ce6742e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training baseline model (TF-IDF + LogisticRegression)...\n",
      "Training complete.\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9873    0.9946    0.9909      4283\n",
      "           1     0.9951    0.9883    0.9917      4695\n",
      "\n",
      "    accuracy                         0.9913      8978\n",
      "   macro avg     0.9912    0.9915    0.9913      8978\n",
      "weighted avg     0.9913    0.9913    0.9913      8978\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[4260   23]\n",
      " [  55 4640]]\n",
      "Saved baseline pipeline to artifacts\\baseline_tfidf_logreg.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "\n",
    "X_train = train_df['content'].tolist()\n",
    "y_train = train_df['label'].values\n",
    "X_test = test_df['content'].tolist()\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "\n",
    "baseline_pipe = Pipeline([\n",
    "('tfidf', TfidfVectorizer(max_features=20000, ngram_range=(1,2))),\n",
    "('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "print(\"Training baseline model (TF-IDF + LogisticRegression)...\")\n",
    "baseline_pipe.fit(X_train, y_train)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "\n",
    "preds = baseline_pipe.predict(X_test)\n",
    "print('\\nClassification report:')\n",
    "print(classification_report(y_test, preds, digits=4))\n",
    "print('\\nConfusion matrix:')\n",
    "print(confusion_matrix(y_test, preds))\n",
    "\n",
    "\n",
    "# save pipeline\n",
    "joblib.dump(baseline_pipe, OUT / 'baseline_tfidf_logreg.joblib')\n",
    "print(f\"Saved baseline pipeline to {OUT / 'baseline_tfidf_logreg.joblib'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e27c8479-5131-4736-a39f-a76acd106be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model probs (real,fake): [0.00224196 0.99775804]\n",
      "Top features and weights:\n",
      " obama -> +0.8346\n",
      " the -> +0.7732\n",
      " our -> +0.5434\n",
      " that -> +0.4682\n",
      " is -> +0.4596\n",
      " this -> +0.3725\n"
     ]
    }
   ],
   "source": [
    "# Purpose: get token-level contributions quickly without LIME\n",
    "\n",
    "# Extract vectorizer and classifier from pipeline\n",
    "vectorizer = baseline_pipe.named_steps['tfidf']\n",
    "clf = baseline_pipe.named_steps['clf']\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "def explain_instance_fast(text: str, num_features: int = 8):\n",
    "    \"\"\"Return top contributing tokens using logistic regression coefficients.\"\"\"\n",
    "    X_vec = vectorizer.transform([text])\n",
    "    contrib = X_vec.multiply(clf.coef_[0]).toarray()[0]\n",
    "    top_idx = contrib.argsort()[-num_features:][::-1]\n",
    "    features = [(feature_names[i], contrib[i]) for i in top_idx]\n",
    "    probs = baseline_pipe.predict_proba([text])[0]\n",
    "    return features, probs\n",
    "\n",
    "# Quick demo\n",
    "sample_text = X_test[0]\n",
    "features, probs = explain_instance_fast(sample_text, num_features=6)\n",
    "print(\"Model probs (real,fake):\", probs)\n",
    "print(\"Top features and weights:\")\n",
    "for feat, w in features:\n",
    "    print(f\" {feat} -> {w:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "921c7838-0e5c-4dbe-a119-191b01407222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator device: CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2249 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating justification for a sample test article...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION: fake | CONF: 0.9977580446745059\n",
      "\n",
      "JUSTIFICATION:\n",
      " This is the most brilliant and scary analysis of where we, as a nation are today Last year a retired Border Patrol Officer by the name of Zach Taylor went on camera to explain the driving force behind the unprecedented surge in illegal immigration happening on our southern border. Taylor went on to note that what was happening at our border was not due to a spur of the moment event, or a humanitarian crisis, but asymmetrical warfare. The surge we saw at the border was apart of a larger more chilling event that served one purpose and one purpose only, to show our enemies that our southern border had been compromised and the government wouldn t do a damn thing about it.The surge we saw at the border was apart of a larger more chilling event that served one purpose and one purpose only, to show our enemies that our southern border had been compromised and the government wouldn t do a damn thing about it.The surge we saw at the border was apart of a larger more chilling event that served one purpose and one purpose only, to show our enemies that our southern border had been compromised and the government wouldn t do a damn thing about it.The surge we saw at the border was apart of a larger\n"
     ]
    }
   ],
   "source": [
    "# Generative justification using Hugging Face (FLAN-T5 small, PyTorch)\n",
    "# ----------------------------\n",
    "# Purpose: Use a small text-to-text model to convert evidence + prediction into a justification.\n",
    "# NOTE: Only requires `transformers` + `torch`, TensorFlow/Keras not needed.\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "MODEL_NAME = \"google/flan-t5-small\"  # compact model for demo\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(\"Generator device:\", \"GPU\" if device == 0 else \"CPU\")\n",
    "\n",
    "# Force PyTorch-only backend\n",
    "generator = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=MODEL_NAME,\n",
    "    device=device,\n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "def make_prompt(article_text: str, label_str: str, confidence: float, feats: list):\n",
    "    \"\"\"Create a concise prompt that includes evidence tokens and asks for a justification.\"\"\"\n",
    "    feat_lines = \"\\n\".join([f\"- '{t[0]}' ({t[1]:+.3f})\" for t in feats]) if feats else \"No salient tokens.\"\n",
    "    prompt = (\n",
    "        f\"Article: {article_text}\\n\"\n",
    "        f\"Prediction: {label_str} (confidence {confidence:.2f})\\n\"\n",
    "        f\"Top contributing tokens:\\n{feat_lines}\\n\\n\"\n",
    "        \"Task: Write a short (2-3 sentence) justification explaining why the model labeled this article as \"\n",
    "        f\"{label_str.upper()}, using the tokens above as evidence.\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def generate_justification(article_text: str):\n",
    "    \"\"\"Predict Fake/Real and generate a justification with evidence tokens.\"\"\"\n",
    "    probs = baseline_pipe.predict_proba([article_text])[0]\n",
    "    pred = baseline_pipe.predict([article_text])[0]\n",
    "    label_str = \"fake\" if pred == 1 else \"real\"\n",
    "    feats, _ = explain_instance_fast(article_text, num_features=6)  # <-- faster version\n",
    "    prompt = make_prompt(article_text, label_str, max(probs), feats)\n",
    "    out = generator(prompt, max_length=120, do_sample=False)[0][\"generated_text\"]\n",
    "    return {\n",
    "        \"label\": label_str,\n",
    "        \"confidence\": float(max(probs)),\n",
    "        \"features\": feats,\n",
    "        \"prompt\": prompt,\n",
    "        \"justification\": out,\n",
    "    }\n",
    "\n",
    "# Demo\n",
    "print(\"\\nGenerating justification for a sample test article...\")\n",
    "res = generate_justification(sample_text)\n",
    "print(\"PREDICTION:\", res[\"label\"], \"| CONF:\", res[\"confidence\"])\n",
    "print(\"\\nJUSTIFICATION:\\n\", res[\"justification\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7ad3d019-0712-4369-9c81-ca3a46e1923b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 50 justifications to artifacts\\justifications.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Cell 7 - Batch generate & save justifications\n",
    "# ----------------------------\n",
    "OUT_JUST = OUT / \"justifications.csv\"\n",
    "\n",
    "rows = []\n",
    "for i in range(min(50, len(X_test))):  # limit to 50 for speed\n",
    "    txt = X_test[i]\n",
    "    r = generate_justification(txt)\n",
    "    rows.append({\n",
    "        \"content\": txt,\n",
    "        \"pred\": r[\"label\"],\n",
    "        \"confidence\": r[\"confidence\"],\n",
    "        \"justification\": r[\"justification\"],\n",
    "        \"features\": str(r[\"features\"])\n",
    "    })\n",
    "\n",
    "pd.DataFrame(rows).to_csv(OUT_JUST, index=False, encoding=\"utf-8\")\n",
    "print(f\"Wrote {len(rows)} justifications to {OUT_JUST}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba6ab09-9354-4aed-8401-2aababe63f97",
   "metadata": {},
   "source": [
    "### Model Deployment in streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e878c87b-85bf-4b4c-a098-15daea736a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Streamlit app written to app.py. Run with: streamlit run app.py\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Cell 8 - Streamlit app\n",
    "# ----------------------------\n",
    "streamlit_app = r\"\"\"\n",
    "import streamlit as st\n",
    "import joblib\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Load baseline model\n",
    "model = joblib.load(\"artifacts/baseline_tfidf_logreg.joblib\")\n",
    "vectorizer = model.named_steps[\"tfidf\"]\n",
    "clf = model.named_steps[\"clf\"]\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "def explain_instance_fast(text: str, num_features: int = 6):\n",
    "    X_vec = vectorizer.transform([text])\n",
    "    contrib = X_vec.multiply(clf.coef_[0]).toarray()[0]\n",
    "    top_idx = contrib.argsort()[-num_features:][::-1]\n",
    "    return [(feature_names[i], contrib[i]) for i in top_idx], model.predict_proba([text])[0]\n",
    "\n",
    "# PyTorch-only generator\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\", device=device, framework=\"pt\")\n",
    "\n",
    "def generate_justification(text: str):\n",
    "    probs = model.predict_proba([text])[0]\n",
    "    pred = model.predict([text])[0]\n",
    "    label = \"fake\" if pred == 1 else \"real\"\n",
    "    feats, _ = explain_instance_fast(text, 6)\n",
    "    feat_lines = \"\\\\n\".join([f\"- {f} ({w:+.3f})\" for f,w in feats])\n",
    "    prompt = f\"Article: {text}\\\\nPrediction: {label} (conf {max(probs):.2f})\\\\nFeatures:\\\\n{feat_lines}\\\\nWrite a short justification.\"\n",
    "    out = generator(prompt, max_new_tokens=120, do_sample=False)[0][\"generated_text\"]\n",
    "    return label, out, feats, probs\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"ðŸ“° Fake News Detector + Justification\")\n",
    "text = st.text_area(\"Paste a news article here:\")\n",
    "\n",
    "if st.button(\"Check\") and text.strip():\n",
    "    label, justification, feats, probs = generate_justification(text)\n",
    "    st.write(f\"Prediction: **{label.upper()}** (Confidence: {max(probs):.2f})\")\n",
    "    st.subheader(\"Justification\")\n",
    "    st.write(justification)\n",
    "    st.subheader(\"Top Features\")\n",
    "    for f,w in feats:\n",
    "        st.write(f\"- {f} ({w:+.3f})\")\n",
    "\"\"\"\n",
    "\n",
    "# Write to app.py\n",
    "with open(\"app.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(streamlit_app)\n",
    "\n",
    "print(\"âœ… Streamlit app written to app.py. Run with: streamlit run app.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a9a96389-13c6-4bf4-a73f-df0208fa4c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is YUMI\n",
      " Volume Serial Number is 7A88-5D93\n",
      "\n",
      " Directory of C:\\Users\\sreya\n",
      "\n",
      "21-09-2025  05:21             1,821 app.py\n",
      "               1 File(s)          1,821 bytes\n",
      "               0 Dir(s)  173,602,611,200 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir app.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622cf533-0aae-43c9-841d-457a483ce5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7161035-568a-4755-a9c5-35574102ee9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
